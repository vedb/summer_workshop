{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency maps\n",
    "This notebook will walk you through computation of a saliency map for an image or a frame of a video. We will use this repository - which you will need to clone into your ~/Code folder! \n",
    "\n",
    "To do this, please call the following at the command line (terminal on phi):\n",
    "```\n",
    "$ cd ~/Code/\n",
    "$ git clone https://github.com/akisatok/pySaliencyMap.git\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import file_io\n",
    "import plot_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Define the Session loader\n",
    "%run ../../code/vedb_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command will run the main useful script in the repo, \n",
    "# which defines some useful functions we will use below.\n",
    "%run ~/Code/pySaliencyMap/pySaliencyMap.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will compute the saliency of some of their demo images. The functions in this library are based on opencv, and thus want BGR image inputs, so we will use opencv to read in an image instead of matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/mark/Code/pySaliencyMap/test2.png'\n",
    "image = cv2.imread(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this repository works, a python *class* is created, which knows about the size of the images or frames that you want to process, and a method of that object actually computes the saliecy of input frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a saliency map generator\n",
    "image_shape = image.shape[:2]\n",
    "saliency_generator = pySaliencyMap(*image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute saliency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"SM\" here is \"Saliency Map\" - so, Saliency Map -> get Saliency Map!\n",
    "saliency_map = saliency_generator.SMGetSM(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the map! The highest values are for the bar that is not the same color as the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "# For display, convert image back to RGB by reversing last axis\n",
    "_ = ax[0].imshow(image[:, :, ::-1])\n",
    "im_handle = ax[1].imshow(saliency_map, cmap='inferno')\n",
    "plt.colorbar(im_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing, for a different image. It's left as an exercise to you to compute saliency maps for the other two example images, if you wish, or to explore the rest of this repo (again, it's here: https://github.com/akisatok/pySaliencyMap)\n",
    "\n",
    "Below, we compute a saliency map for frames of VEDB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/mark/Code/pySaliencyMap/test3.jpg'\n",
    "# The functions in this library want BGR image inputs, so use opencv \n",
    "# to read in image\n",
    "image = cv2.imread(file_path)\n",
    "# Create a saliency map generator\n",
    "image_shape = image.shape[:2]\n",
    "saliency_generator = pySaliencyMap(*image_shape)\n",
    "# \"SM\" here is \"Saliency Map\" - so, Saliency Map -> get Saliency Map!\n",
    "saliency_map = saliency_generator.SMGetSM(image)\n",
    "# Show map\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "# For display, convert image back to RGB by reversing last axis\n",
    "_ = ax[0].imshow(image[:, :, ::-1])\n",
    "im_handle = ax[1].imshow(saliency_map, cmap='inferno')\n",
    "plt.colorbar(im_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing saliency of regions in movie frames\n",
    "\n",
    "## List available sessions of VEDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = sorted(glob.glob('/home/data/vedb/*'))\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab one session\n",
    "ses = Session(folder=sessions[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses.folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load frames in at limited resolution (300 x 400 pixels)\n",
    "# Note that we're loading in BGR color format! \n",
    "\n",
    "# Also, we will specify frames rather than time to make this \n",
    "# easier to map to gaze\n",
    "start = int(30 * 60 * 3.8) # 3.2 mins in\n",
    "fin = start + 30 # 30 frames later\n",
    "world_time, world_video = ses.load('world_camera', \n",
    "                                  size=(300,400), \n",
    "                                  color='bgr', \n",
    "                                  time_idx=None,\n",
    "                                  frame_idx=(start, fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 relevant dimensions for frame shape are these:\n",
    "image_shape = world_video.shape[1:3]\n",
    "frame_number = 0\n",
    "saliency_generator = pySaliencyMap(*image_shape)\n",
    "# \"SM\" here is \"Saliency Map\" - so, Saliency Map -> get Saliency Map!\n",
    "saliency_map = saliency_generator.SMGetSM(world_video[frame_number])\n",
    "# Show map\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "# For display, convert image back to RGB by reversing last axis\n",
    "_ = ax[0].imshow(world_video[frame_number, :, :, ::-1])\n",
    "im_handle = ax[1].imshow(saliency_map, cmap='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now that we have saliency, let's have a look at where the participant was looking on this frame. We have gaze data for each session in \n",
    "\n",
    "`/home/data/vedb_processed/<folder>/gaze_2D_PL_left_eye.csv`\n",
    "and \n",
    "`/home/data/vedb_processed/<folder>/gaze_2D_PL_right_eye.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gaze file with pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze = pd.read_csv('/home/data/vedb_processed/%s/gaze_2D_PL_left_eye.csv'%ses.folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot gaze on the saliency map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdim, hdim = image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze.norm_pos_y[start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note: When plotting gaze (or any quantity) on top of an image, you have to make sure you get the Y coordinate correct. Convention for images is to have zero at the top of the Y axis; so make sure your Y variable is correctly displayed (Sometimes y values follow graph convention instead of image convention, and go UP fromthe bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show map\n",
    "j = 22\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "# For display, convert image back to RGB by reversing last axis\n",
    "_ = ax[0].imshow(world_video[frame_number + j, :, :, ::-1])\n",
    "_ = ax[1].imshow(saliency_map, cmap='inferno')\n",
    "# Gaze is in green \n",
    "ax[1].plot(gaze.norm_pos_x[start+j] * hdim, (gaze.norm_pos_y[start+j]) * vdim, 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with these saliency maps a bit more! \n",
    "\n",
    "Ideas for exploration: \n",
    "* compute saliency for more frames\n",
    "* compute saliency for other sessions (other videos)\n",
    "* See if you can think of a way to assess how good the saliency map is at capturing where people look - how would you EVALUATE the model? what would you compute? \n",
    "\n",
    "\n",
    "Ideas for final projects:\n",
    "* How is saliency related to the presence of `<x>`?  where `<x>` can be bodies, objects, or other labeled things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer_workshop",
   "language": "python",
   "name": "summer_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
